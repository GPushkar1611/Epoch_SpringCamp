{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba93e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f13dd",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Task\n",
    "\n",
    "We are provided with a toy dataset that classifies drinks into three categories: **Wine**, **Beer**, and **Whiskey**. The classification is based on three features:\n",
    "\n",
    "1. **Alcohol Content (%)**\n",
    "2. **Sugar Content (g/L)**\n",
    "3. **Color** (0: light, 1: dark)\n",
    "\n",
    "Our goal is to build a **Decision Tree Classifier** that predicts the type of drink given its features.\n",
    "\n",
    "#### Toy Dataset:\n",
    "| Alcohol Content (%) | Sugar (g/L) | Color (0: light, 1: dark) | Drink Type |\n",
    "|----------------------|-------------|---------------------------|------------|\n",
    "| 12.0                | 1.5         | 1                         | Wine       |\n",
    "| 5.0                 | 2.0         | 0                         | Beer       |\n",
    "| 40.0                | 0.0         | 1                         | Whiskey    |\n",
    "| 13.5                | 1.2         | 1                         | Wine       |\n",
    "| 4.5                 | 1.8         | 0                         | Beer       |\n",
    "| 38.0                | 0.1         | 1                         | Whiskey    |\n",
    "| 11.5                | 1.7         | 1                         | Wine       |\n",
    "| 5.5                 | 2.3         | 0                         | Beer       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5f68277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "data = [\n",
    "    [12.0, 1.5, 1, 'Wine'],\n",
    "    [5.0, 2.0, 0, 'Beer'],\n",
    "    [40.0, 0.0, 1, 'Whiskey'],\n",
    "    [13.5, 1.2, 1, 'Wine'],\n",
    "    [4.5, 1.8, 0, 'Beer'],\n",
    "    [38.0, 0.1, 1, 'Whiskey'],\n",
    "    [11.5, 1.7, 1, 'Wine'],\n",
    "    [5.5, 2.3, 0, 'Beer']\n",
    "]\n",
    "\n",
    "# Map string labels to integers for processing\n",
    "label_map = {'Wine': 0, 'Beer': 1, 'Whiskey': 2}\n",
    "\n",
    "# Replace label strings with corresponding integers\n",
    "for row in data:\n",
    "    row[3] = label_map[row[3]]\n",
    "\n",
    "# Convert list of lists to NumPy array with float values\n",
    "data = np.array(data, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb4191ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features (X) and labels (y)\n",
    "X = data[:, :-1]            # All columns except the last one as features\n",
    "y = data[:, -1].astype(int) # Last column as labels, converted to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b938c3",
   "metadata": {},
   "source": [
    "### Gini Impurity Calculation\n",
    "\n",
    "This function calculates the **Gini impurity** for a set of class labels, which measures node \"purity\" in decision trees.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{n} (p_i)^2\n",
    "$$\n",
    "Where \\( p_i \\) is the probability of class \\( i \\) in the labels.\n",
    "\n",
    "#### Steps:\n",
    "1. **Count Classes**: `np.unique(labels, return_counts=True)` gets frequency counts for each class\n",
    "2. **Calculate Probabilities**: Convert counts to probabilities\n",
    "3. **Compute Impurity**: Subtract sum of squared probabilities from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68f0e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(labels):\n",
    "    _, counts = np.unique(labels, return_counts=True)  # Get counts of each class (ignore the actual label values with '_')\n",
    "    probabilities = counts / counts.sum()  # Class probabilities\n",
    "    gini = 1 - np.sum(probabilities ** 2)  # Gini impurity formula\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb089d",
   "metadata": {},
   "source": [
    "### Finding the Best Split in a Decision Tree\n",
    "\n",
    "This function identifies the optimal feature and threshold to split the data, minimizing the weighted Gini impurity.\n",
    "\n",
    "#### Key Formula (Weighted Gini):\n",
    "$$\n",
    "\\text{Weighted Gini} = \\left(\\frac{n_{\\text{left}}}{n_{\\text{total}}}\\right)Gini_{\\text{left}} + \\left(\\frac{n_{\\text{right}}}{n_{\\text{total}}}\\right)Gini_{\\text{right}}\n",
    "$$\n",
    "\n",
    "#### How It Works:\n",
    "1. **Initialization**:  \n",
    "   - Starts with infinite `best_gini` (seeking lower values)\n",
    "   - Tracks best feature index and threshold\n",
    "\n",
    "2. **Feature Iteration**:  \n",
    "   - Examines each feature (Alcohol%, Sugar, Color)\n",
    "   - Generates candidate thresholds between sorted unique values\n",
    "\n",
    "3. **Threshold Evaluation**:  \n",
    "   - Splits data using `feature ≤ threshold`\n",
    "   - Skips invalid splits (empty left/right subsets)\n",
    "   - Computes weighted Gini impurity for valid splits\n",
    "\n",
    "4. **Optimal Selection**:  \n",
    "   - Retains the split with lowest weighted Gini\n",
    "   - Returns best feature index, threshold, and impurity score\n",
    "\n",
    "#### Key Characteristics:\n",
    "- Exhaustively checks all possible splits\n",
    "- Uses midpoints between consecutive values as thresholds\n",
    "- Prefers splits that create the purest child nodes\n",
    "- Runtime complexity: O(features × thresholds × samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ef47710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    n_samples, n_features = X.shape  # Get number of samples and features\n",
    "    best_gini = float('inf')         # Initialize best Gini as infinity (we want to minimize it)\n",
    "    best_feature = None              # Track the best feature to split on\n",
    "    best_threshold = None            # Track the best threshold for the split\n",
    "\n",
    "    current_gini = gini_impurity(y)  # Gini impurity of the current node (before splitting)\n",
    "    \n",
    "    for feature_idx in range(n_features):\n",
    "        # Get sorted unique values for this feature\n",
    "        feature_values = np.sort(np.unique(X[:, feature_idx]))\n",
    "        \n",
    "        # Compute midpoints between consecutive unique values as candidate thresholds\n",
    "        if len(feature_values) > 1:\n",
    "            thresholds = [(feature_values[i] + feature_values[i+1]) / 2 \n",
    "                          for i in range(len(feature_values) - 1)]\n",
    "        else:\n",
    "            thresholds = [feature_values[0]]  # Only one unique value — use it as threshold\n",
    "            \n",
    "        for threshold in thresholds:\n",
    "            # Split data into left and right based on threshold\n",
    "            left_mask = X[:, feature_idx] <= threshold\n",
    "            right_mask = ~left_mask  # Everything else goes to the right\n",
    "            \n",
    "            # Skip split if either side is empty\n",
    "            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                continue\n",
    "                \n",
    "            y_left, y_right = y[left_mask], y[right_mask]\n",
    "            \n",
    "            # Calculate Gini impurity for both sides\n",
    "            gini_left = gini_impurity(y_left)\n",
    "            gini_right = gini_impurity(y_right)\n",
    "            \n",
    "            # Compute weighted average of the two impurities\n",
    "            n_left, n_right = len(y_left), len(y_right)\n",
    "            weighted_gini = (n_left * gini_left + n_right * gini_right) / n_samples\n",
    "            \n",
    "            # Keep track of the best (lowest impurity) split\n",
    "            if weighted_gini < best_gini:\n",
    "                best_gini = weighted_gini\n",
    "                best_feature = feature_idx\n",
    "                best_threshold = threshold\n",
    "\n",
    "    return best_feature, best_threshold, best_gini  # Return the best split found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "408686d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # Index of feature used for splitting\n",
    "        self.threshold = threshold          # Threshold value for the split\n",
    "        self.left = left                    # Left child node (subtree)\n",
    "        self.right = right                  # Right child node (subtree)\n",
    "        self.value = value                  # Class label if it's a leaf node\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None  # Returns True if node is a leaf (i.e., has a class label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dab6b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(y):\n",
    "    values, counts = np.unique(y, return_counts=True)  # Get unique labels and their counts\n",
    "    return values[np.argmax(counts)]  # Return the label with the highest count (majority class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d79f02",
   "metadata": {},
   "source": [
    "### Decision Tree Construction Process\n",
    "\n",
    "This recursive function builds a decision tree by splitting nodes until stopping criteria are met.\n",
    "\n",
    "#### Node Types:\n",
    "- **Decision Node**:  \n",
    "  Contains:\n",
    "  - `feature_index`: Feature used for splitting (e.g., Alcohol%)\n",
    "  - `threshold`: Split boundary (e.g., ≤12.5%)\n",
    "  - `left/right`: Child nodes\n",
    "- **Leaf Node**:  \n",
    "  Contains `value`: Final class prediction (Wine/Beer/Whiskey)\n",
    "\n",
    "#### Workflow:\n",
    "1. **Check Stopping Conditions**:\n",
    "   - All samples same class → Create leaf\n",
    "   - Reached depth limit → Create majority-class leaf\n",
    "   - Insufficient samples → Create majority-class leaf\n",
    "\n",
    "2. **Find Optimal Split**:\n",
    "   - Uses `best_split()` to minimize weighted Gini impurity\n",
    "\n",
    "3. **Create Subtrees**:\n",
    "   - Left branch: Samples where `feature ≤ threshold`\n",
    "   - Right branch: Remaining samples\n",
    "   - Recursively repeat process for each branch\n",
    "\n",
    "#### Control Parameters:\n",
    "| Parameter | Purpose |\n",
    "|-----------|---------|\n",
    "| `max_depth=5` | Prevents overcomplex trees |\n",
    "| `min_samples=2` | Avoids splits on tiny groups |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83255bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, depth=0, max_depth=5, min_samples=2):\n",
    "    n_samples, n_features = X.shape  # Get number of samples and features\n",
    "    \n",
    "    # Stop if all labels are the same (pure node)\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return Node(value=y[0])  # Make leaf with the common label\n",
    "    \n",
    "    # Stop if maximum depth reached or not enough samples to split\n",
    "    if depth >= max_depth or n_samples < min_samples:\n",
    "        return Node(value=majority_label(y))  # Make leaf with majority label\n",
    "    \n",
    "    # Find the best feature and threshold to split on\n",
    "    feature_idx, threshold, _ = best_split(X, y)\n",
    "    \n",
    "    # If no valid split was found (e.g. constant features)\n",
    "    if feature_idx is None:\n",
    "        return Node(value=majority_label(y))  # Make leaf with majority label\n",
    "    \n",
    "    # Split dataset into left and right branches\n",
    "    left_mask = X[:, feature_idx] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "    \n",
    "    # Safety check in case split is invalid (shouldn't usually happen)\n",
    "    if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "        return Node(value=majority_label(y))  # Make leaf with majority label\n",
    "    \n",
    "    # Recursively build left and right subtrees\n",
    "    left_subtree = build_tree(X[left_mask], y[left_mask], depth + 1, max_depth, min_samples)\n",
    "    right_subtree = build_tree(X[right_mask], y[right_mask], depth + 1, max_depth, min_samples)\n",
    "    \n",
    "    # Return decision node with split info and subtrees\n",
    "    return Node(feature_index=feature_idx, threshold=threshold, \n",
    "                left=left_subtree, right=right_subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e80d670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(node, x):\n",
    "    # If current node is a leaf, return its value (class label)\n",
    "    if node.is_leaf_node():\n",
    "        return node.value\n",
    "\n",
    "    # Recursively follow the left or right subtree based on threshold\n",
    "    if x[node.feature_index] <= node.threshold:\n",
    "        return predict_sample(node.left, x)\n",
    "    else:\n",
    "        return predict_sample(node.right, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba86b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, X):\n",
    "    # Apply the predict_sample function to each sample in X and return the predicted labels\n",
    "    return np.array([predict_sample(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4dc6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c44aac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([\n",
    "    [6.0, 2.1, 0],   # Expected: Beer\n",
    "    [39.0, 0.05, 1], # Expected: Whiskey\n",
    "    [13.0, 1.3, 1]   # Expected: Wine\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2bcf9",
   "metadata": {},
   "source": [
    "### Decision Tree Prediction Process\n",
    "\n",
    "This code uses the trained decision tree to classify test data into drink types:\n",
    "\n",
    "1. **Tree Traversal**:  \n",
    "   For each test sample, the tree is traversed from root to leaf:\n",
    "   - Decision nodes check features against thresholds (e.g., \"Alcohol ≤ 40.0?\")\n",
    "   - Follow left/right branches until reaching a leaf node\n",
    "\n",
    "2. **Label Conversion**:  \n",
    "   Converts numerical predictions to drink names using:\n",
    "   `types = {0: \"Wine\", 1: \"Beer\", 2: \"Whiskey\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6c65340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Drink Types: ['Beer', 'Whiskey', 'Wine']\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test data\n",
    "predicted_labels = predict(tree, test_data)\n",
    "\n",
    "# Map numeric labels back to the original drink types\n",
    "types = {0: \"Wine\", 1: \"Beer\", 2: \"Whiskey\"}\n",
    "predicted_types = [types[label] for label in predicted_labels]\n",
    "\n",
    "# Print the predicted drink types\n",
    "print(\"Predicted Drink Types:\", predicted_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
